{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8ef05a-3173-464e-9454-aea0ce6d5757",
   "metadata": {},
   "source": [
    "## Imports and seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2ad7c-5d5c-402b-a0ad-4bc95aff916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a57287-cd17-4087-a220-99c6df9e1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.interpolate import griddata\n",
    "from multitask_basis_func_gp.gen_funcs import *\n",
    "from multitask_basis_func_gp.basis_funcs import *\n",
    "from multitask_basis_func_gp.utils import *\n",
    "from multitask_basis_func_gp.visual_utils import *\n",
    "from multitask_basis_func_gp.multitask_basis_func_gp import MultitaskBasisFuncGPModel\n",
    "from multitask_basis_func_gp.dot_product_prediction_strategy import *\n",
    "\n",
    "from gpytorch.distributions import MultitaskMultivariateNormal, MultivariateNormal\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import MaternKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea28aea-1293-4bfb-81f4-a62157e376a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_path = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629aba0c-45fa-4aad-824a-c9fd21dccbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f644201-f813-4b8c-a74e-98b4ef20543e",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9665b-4937-461d-a343-3ec0f8e94631",
   "metadata": {},
   "source": [
    "Recall that we model the KPI's dependence on time $t$ (could be day/hour/week) and dual $d$ as KPI(t, d) = SUM coeff(t) * basis_func(d) + noise.\n",
    "\n",
    "That is, $KPI(t,d) = \\sum_{i=1}^F \\alpha_i(t)f_i(d) + \\epsilon$ for deterministic monotonic basis functions $f_i(d)$ with $i = 1 \\to F$ and iid scalar Gaussian noise $\\epsilon \\sim N(0, \\sigma_{up}^2)$. We model the F-dimensional vector of coefficients $(\\alpha_i(t))_i$ as a GP, denote by coeffs or coeff in the code. We will first fit a prior using Excalibur curves and then use actual historical data to make updates. \n",
    "\n",
    "IF we choose dual $d_0$ at time $t_0$, then we observe only $KPI(t_0, d_0)$, we do not immediately have access to good counterfactuals. So we want to update the WHOLE F-dimensional vector-valued GP $t \\mapsto [\\alpha_i(t)]_i$ using information about ONLY its dot product with the vector $[f_i(d_0)]_i$ at time $t_0$, because this is exactly $KPI(t_0, d_0) + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48edbb08-59ae-45e2-a435-faca9aadd887",
   "metadata": {},
   "source": [
    "### Declare major parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f79be6-e890-4610-bd84-9328c3e30ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various hand-picked parameters\n",
    "\n",
    "# Important parameters\n",
    "max_time = 24 # CHANGE TO 7 OR 0 IF WORKING WITH DAYS\n",
    "train_t = torch.arange(max_time).float() # Setting all times to be used for training data\n",
    "use_relu = True # Whether to use ReLU to make the occasional negative coefficients positive, or to rely on the actual prediction itself\n",
    "\n",
    "# update_sigma is the variance for the GP update, same as sigma_up\n",
    "y_scale = 1.0 # Scale of the output data, typically 1.0\n",
    "init_update_sigma = 0.1*y_scale # Initial value for the noise in the GP update, typically 0.1*y_scale\n",
    "# init_update_sigma = 0.8\n",
    "\n",
    "\"\"\"Notes on update_sigma:\n",
    "update_sigma is currently set by hand in the notebook, change to automated grid search later\n",
    "Recall this is the stddev of epsilon, the scalar noise\n",
    "KEEP IN MIND: update_sigma controls how much faith in signal vs faith in prior model we have\n",
    "1/500*kpi_max to kpi_max, typical range for update_sigma\n",
    "I suspect that best results will be between 0.01*kpi_max and 0.3*kpi_max\n",
    "\"\"\"\n",
    "\n",
    "# For optimization\n",
    "verbose = True # SET TO FALSE WHEN DEPLOYING\n",
    "per_compute_verbose = False\n",
    "# Parameters for prior hyperparameter gradient descent \n",
    "prior_lr = 0.1\n",
    "prior_sched_gamma = 0.999\n",
    "# Parameters for posterior hyperparameter gradient descent \n",
    "post_lr = 0.1\n",
    "post_sched_gamma = 0.999\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_prior_train_iter = 2 if smoke_test else 50\n",
    "num_post_train_iter = 50\n",
    "\n",
    "# More technical parameters\n",
    "dual_scale_factor = 6 # Scale duals by dual_scale_factor*dual_max to allow wiggle room for extrapolation beyond dual_max\n",
    "grid_size = 500 # Grid of duals used to generated interpolated data\n",
    "num_basis = 45 # number of basis functions, this is F\n",
    "\n",
    "\n",
    "rank = 45 # Rank of covariance matrix for coefficients alpha \n",
    "\"\"\" Notes on rank: In practice, it doesn't seem to matter much.\n",
    "I think this is the rank you get after hyperparameter optimization in gpytorch and before using excalibur data.\n",
    "It's not the rank you would get after prior fitting (which is in our case GP inference over direct coefficients learnt from excalibur)\n",
    "\"\"\"\n",
    "\n",
    "# Choose mean and covariance module for the vector-valued coeff GP\n",
    "mean_module = gpytorch.means.MultitaskMean(ConstantMean(), num_tasks=num_basis)\n",
    "covar_module = gpytorch.kernels.MultitaskKernel(MaternKernel(), num_tasks=num_basis, rank=rank)\n",
    "# If we want cyclic relation across hours, then we might want a cyclic distance function for the kernel\n",
    "\n",
    "# Parameters for discrete sigmoid basis functions\n",
    "# ONLY change if you are changing the basis functions and have looked into how to do that\n",
    "gap = 0.02 # Size of non flat part of discrete sigmoid functions, slightly less than 1/num_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7a342-85b6-459c-b32a-f646e3b26dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up basis functions\n",
    "start_num = num_basis\n",
    "d_start_range = np.linspace(0, 1, start_num)\n",
    "steep_d_sigmoid_basis = BasisFuncs(discrete_sigmoid_gap_based)\n",
    "for start in d_start_range:\n",
    "    steep_d_sigmoid_basis.append([start, gap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6090c-1ddb-43b9-b243-2fba0c080ec5",
   "metadata": {},
   "source": [
    "### Set up training coefficients and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39337ed-60fa-4edf-af48-5e37dbcd6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training coefficients\n",
    "# We use linear regression over dual-kpi grid to get training coefficients coeff_i(t) for each time t\n",
    "\"\"\"IMPORTANT UTILITY FUNCTION\"\"\"\n",
    "train_t, train_coeffs = gen_true_coeffs(df_exc, scale_df, inst_id, train_t, dual_scale_factor, grid_size, basis_funcs = steep_d_sigmoid_basis, kpi=kpi)\n",
    "\n",
    "# Shape = num_train_times, num_basis\n",
    "print(inst_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adcb139-5856-432c-86b5-e2915d4f3d44",
   "metadata": {},
   "source": [
    "## Initialize model and train prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d16714-bd03-4479-a26b-b0d1fedb6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up \"likelihood\" (essentially takes GP output distribution f(x) and gives an output distribution by adding appropriate noise)\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_basis)\n",
    "\n",
    "# Initialize model\n",
    "model = MultitaskBasisFuncGPModel(train_t, train_coeffs, num_basis, likelihood, init_update_sigma, steep_d_sigmoid_basis, max_time, mean_module, covar_module)\n",
    "\n",
    "\"\"\"\n",
    "You can add kpi_observed and dual_chosen and time_observed right here as well if you don't have train_coeffs. \n",
    "In that case, just skip hyperparameter optimization.\n",
    "You can change mean_module to a custom mean and covar_module as well if you want, in case you have some general prior knowledge.\n",
    "model.update_inputs = ...\n",
    "model.update_basis_inputs = ...\n",
    "model.update_label = ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e30f7-4468-4096-aedc-82b2bd3bcc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "prior_hyperparam_optim(model, likelihood, train_t, train_coeffs, num_prior_train_iter, lr = prior_lr, scheduler_gamma = prior_sched_gamma, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53981ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "model.log_update_sigma.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e633c8-a18f-4efe-9d0f-31eee75dce26",
   "metadata": {},
   "source": [
    "### Check prior graphs against excalibur graphs (visualization only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2dbb2-ec0d-467f-a9da-7c66912a1be0",
   "metadata": {},
   "source": [
    "There will be difference in the power (magnitude) but not shape of the dual-kpi graphs from the prior and the excalibur curve data. That's because there is a high variation in power of the hourly dual-kpi curves as the hours progress, while the GP prior fit \"smooths out\" this variation. Yannis believes (and I agree) that this is because of poor calibration of excalibur curves, which only happens at the day level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d39f6-17c2-46cc-a12f-7f493579a5ce",
   "metadata": {},
   "source": [
    "#### Dual-KPI graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd021eb-4f28-4943-8dec-6585a8b20d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose time to display dual-kpi curve for\n",
    "prior_check_time = 5\n",
    "\n",
    "# Get test data\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Get kpi values for the scaled dual grid\n",
    "scaled_dual_x, kpi_true, scale = gen_unif_grid_dual_kpi(df_exc, scale_df, inst_id, prior_check_time, dual_scale_factor, grid_size, kpi=kpi)\n",
    "kpi_prior = gen_kpi_vals_single_time(model, likelihood, steep_d_sigmoid_basis, scaled_dual_x,\\\n",
    "                                     prior_check_time, kpi=kpi, mode = \"prior\", use_relu=use_relu).detach()\n",
    "\n",
    "# Plot dual-kpi graph for chosen time\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(scaled_dual_x*scale, kpi_true, color = \"orange\")\n",
    "# ax.plot(scaled_dual_x*scale, kpi_prior)\n",
    "ax.set_xlabel(\"Dual (Scaled)\")\n",
    "ax.set_ylabel(kpi)\n",
    "# ax.legend([\"Excalibur Curve\", \"Prior Prediction\"])\n",
    "plt.title(\"Dual-KPI Excalibur Curve, Instance Id \" + str(inst_id) + \", Time \" + str(prior_check_time) + \"\")\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd8892-cc01-4f9e-b54f-9d49f13165e4",
   "metadata": {},
   "source": [
    "#### Coeff-time graphs (sanity check for \"power smoothing\" due to GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb1a97-4d99-4b98-ac52-944c96e40730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose coefficient index to plot coeff-time graph for\n",
    "check_coeffs_idx = 1\n",
    "\n",
    "# Set eval mode for safety\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "prior_coeffs = model(train_t).mean.detach().numpy()\n",
    "likelihood(model(train_t))\n",
    "show_coeff_time_graph(inst_id, train_t, prior_coeffs, train_coeffs, check_coeffs_idx,\\\n",
    "                      \"Prior Coefficient Variation\", \"Excalibur Coefficient Variation\", use_relu=use_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810edfb-c71e-47e9-80ec-bb36135fc44d",
   "metadata": {},
   "source": [
    "## Set up update data and make posterior update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662212ca-f69c-4bee-9d79-c95b90178b4f",
   "metadata": {},
   "source": [
    "### Set up update data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9ab5b-36db-40ee-9360-93ecec16f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_inputs, update_basis_inputs, update_labels =  get_real_vals(df_real, scale_df, inst_id, dual_scale_factor)\n",
    "model.update_inputs, model.update_basis_inputs, model.update_labels = update_inputs, update_basis_inputs, update_labels\n",
    "\n",
    "# Toggle to slice for smaller inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39fa45a-5164-4431-be06-407d2c6ea378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior vals\n",
    "check_time = 11\n",
    "\n",
    "# Get kpi values corresponding to given dual grid\n",
    "\"\"\"IMPORTANT UTILITY FUNCTION\"\"\"\n",
    "kpi_post = gen_kpi_vals_single_time(model, likelihood, steep_d_sigmoid_basis, scaled_dual_x, check_time, kpi=kpi, mode = \"posterior\", use_relu=use_relu)\n",
    "kpi_prior = gen_kpi_vals_single_time(model, likelihood, steep_d_sigmoid_basis, scaled_dual_x, check_time, kpi=kpi, mode = \"prior\", use_relu=use_relu)\n",
    "\n",
    "# Get prediction for single dual used\n",
    "dual_chosen = 0.4\n",
    "dual_used = torch.tensor([dual_chosen], dtype=torch.float)\n",
    "kpi_post_pred = gen_kpi_vals_single_time(model, likelihood, steep_d_sigmoid_basis, dual_used, check_time, kpi=kpi, mode = \"posterior\", use_relu=use_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4cd36-54dc-447a-a5b6-3194cf667dbc",
   "metadata": {},
   "source": [
    "### Compare prior and posterior curves (visualization only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ce3a8-0472-4d0b-84e5-86cf3a0388ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change update sigma if you want\n",
    "model.log_update_sigma = torch.nn.Parameter(torch.tensor(math.log(0.5)))\n",
    "model.update_inputs, model.update_basis_inputs, model.update_labels = update_inputs[:5], update_basis_inputs[:5], update_labels[:5]\n",
    "# Shows prior and posterior dual-kpi curves\n",
    "show_prior_post_dual_kpi_curves(model, likelihood, scale_df, inst_id, model.update_inputs, dual_scale_factor, grid_size, steep_d_sigmoid_basis, kpi=kpi, use_relu=use_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc3f5e",
   "metadata": {},
   "source": [
    "### Find optimal update_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c96002",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# torch clear memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try loop that tries to use gen_kpi_vals for various values of kpi_scale to determine init_update_sigma until no errors are thrown\n",
    "from multitask_basis_func_gp.utils import *\n",
    "min_kpi_scaling_power = -2\n",
    "max_kpi_scaling_power = 1\n",
    "kpi_scalings = torch.logspace(-3, 0, 15)\n",
    "\n",
    "sigma_grid_size = 30\n",
    "kpi_scaling = get_kpi_scaling(df_real, scale_df, inst_id, kpi_scale, model, likelihood, dual_scale_factor, steep_d_sigmoid_basis, kpi=\"Spend\", use_relu = use_relu,\\\n",
    "                    min_power = min_kpi_scaling_power, kpi_scalings = kpi_scalings, verbose = True)\n",
    "\n",
    "print(kpi_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034544cd-904e-46bd-9406-f4ae2a9df73c",
   "metadata": {},
   "source": [
    "#### Generate and plot error array for given update_sigma grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663d862-7a4b-433d-96a6-788d95196676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sigma grid\n",
    "sigma_grid = torch.logspace(min_kpi_scaling_power, max_kpi_scaling_power, sigma_grid_size)*kpi_scale*kpi_scaling\n",
    "\n",
    "# Make sure update data is empty\n",
    "empty_tensor = torch.tensor([], dtype=torch.float)\n",
    "model.update_inputs, model.update_basis_inputs, model.update_labels = empty_tensor, empty_tensor, empty_tensor\n",
    "\n",
    "\"\"\"IMPORTANT UTILITY FUNCTION\"\"\"\n",
    "error_array = gen_error_array(None, df_real, scale_df, inst_id, model, likelihood,\\\n",
    "                sigma_grid, dual_scale_factor, steep_d_sigmoid_basis, \\\n",
    "                              kpi=kpi, use_relu = use_relu, verbose = verbose, per_compute_verbose =per_compute_verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32d60e-542c-4a0c-9e9e-7de3f7e46a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up update data\n",
    "model.update_inputs, model.update_basis_inputs, model.update_labels =  get_real_vals(df_real, scale_df, inst_id, dual_scale_factor)\n",
    "\n",
    "# Generate error array for historical training data\n",
    "error_array_test = gen_error_array(df_real, df_real_test, scale_df, inst_id, model, likelihood,\\\n",
    "                sigma_grid, dual_scale_factor, steep_d_sigmoid_basis, \\\n",
    "                              kpi=kpi, use_relu = use_relu, verbose = verbose, per_compute_verbose =per_compute_verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f3107-7fa2-4a1e-a8d9-5ffb27e490c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sigma values against errors\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(torch.log10(sigma_grid), error_array.detach())\n",
    "ax.plot(torch.log10(sigma_grid), error_array_test.detach())\n",
    "ax.legend([\"Training error\", \"Test error (next week)\"])\n",
    "ax.set_title(\"InstanceId \" + str(inst_id))\n",
    "ax.set_xlabel(\"update_sigma\")\n",
    "ax.set_ylabel(\"prediction_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1996772-0997-4ab4-a189-4a0b3f197032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sigma with min error\n",
    "best_sigma = sigma_grid[torch.argmin(error_array)]\n",
    "test_error = compute_post_pred_error(df_real, df_real_test, scale_df, inst_id, model, likelihood, best_sigma, dual_scale_factor, steep_d_sigmoid_basis, kpi=kpi, use_relu = use_relu, verbose = verbose)\n",
    "print(\"Best sigma for training data: \", best_sigma)\n",
    "print(\"Test error for best sigma: \", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb281c-334d-45b2-b13a-3b4ae652432b",
   "metadata": {},
   "source": [
    "#### Find optimal sigma using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28c53a-8496-4022-9710-45206ab2d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sigma grid\n",
    "# PLAY AROUND WITH THIS CHOICE IN DIFFERENT SCENARIOS\n",
    "sigma_grid = torch.logspace(-2, 1, 30)*kpi_scale*kpi_scaling\n",
    "\n",
    "# Set up update data\n",
    "model.update_inputs, model.update_basis_inputs, model.update_labels =  get_real_vals(df_real, scale_df, inst_id, dual_scale_factor)\n",
    "\n",
    "# Directly get update_sigma with min error on real data\n",
    "best_sigma = best_update_sigma_grid_search(None, df_real, scale_df, inst_id, model, likelihood,\\\n",
    "                sigma_grid, dual_scale_factor, steep_d_sigmoid_basis,\\\n",
    "               kpi=kpi, use_relu = use_relu, verbose = verbose, per_compute_verbose = per_compute_verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2ee59-e930-4e60-839d-2e23ac62945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sigma with min error\n",
    "best_sigma = sigma_grid[torch.argmin(error_array)]\n",
    "test_error = compute_post_pred_error(df_real, df_real_test, scale_df, inst_id, model, likelihood, best_sigma, dual_scale_factor, steep_d_sigmoid_basis, kpi=kpi, use_relu = use_relu, verbose = verbose)\n",
    "print(\"Best sigma for training data: \", best_sigma)\n",
    "print(\"Test error for best sigma: \", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ba789-8335-4c71-b182-e3c23d3d5652",
   "metadata": {},
   "source": [
    "#### Find optimal sigma using gradient based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1a461-5f78-44ea-919c-8688b3843183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up empty update data\n",
    "empty_tensor = torch.tensor([], dtype=torch.float)\n",
    "model.update_inputs, model.update_basis_inputs, model.update_labels = empty_tensor, empty_tensor, empty_tensor\n",
    "\n",
    "# Set initial update_sigma\n",
    "init_update_sigma = kpi_scale*kpi_scaling\n",
    "\n",
    "# Implicitly update model's update_sigma using gradient descent over the same MSE target used above\n",
    "loss_array = best_update_sigma_grad_opt(None, df_real, scale_df, inst_id, model, likelihood, dual_scale_factor, steep_d_sigmoid_basis, 30,\\\n",
    "                          lr = post_lr, scheduler_gamma = post_sched_gamma,\\\n",
    "                      init_update_sigma = init_update_sigma, verbose = verbose, per_compute_verbose = per_compute_verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3,4]).float()\n",
    "idx_1 = a.eq(4)\n",
    "idx_1.nonzero().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_array)\n",
    "ax.set_title(\"Error over iterations, InstanceId \" + str(inst_id))\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51158df4-78f4-4626-8b93-ad8cc5c99c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sigma with min error\n",
    "best_sigma = model.update_sigma\n",
    "test_error = compute_post_pred_error(df_real, df_real_test, scale_df, inst_id, model, likelihood, best_sigma, dual_scale_factor, steep_d_sigmoid_basis, kpi=kpi, use_relu = use_relu, verbose = verbose)\n",
    "print(\"Best sigma for training data: \", best_sigma)\n",
    "print(\"Test error for best sigma: \", test_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
